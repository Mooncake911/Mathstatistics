__R-squared__ является, пожалуй, самым важным измерением, 
полученным в этой сводке. R-квадрат — это мера того, 
какая часть зависимой переменной объясняется изменениями 
в наших независимых переменных. В процентном соотношении 
0,338 будет означать, что наша модель объясняет 33,8% 
изменения нашей зависимой переменной.
***
__Adj. R-squared__ важен для анализа эффективности
нескольких зависимых переменных в модели. Свойство линейной регрессией 
заключается в том, что значение R-квадрата нашей модели никогда 
не будет уменьшаться с дополнительными переменными, а только будет 
равно или выше. Таким образом, модель может выглядеть более точной 
с несколькими переменными, даже если они плохо влияют друг на друга. 
Скорректированный R-квадрат наказывает формулу R-квадрата в зависимости 
от количества переменных, поэтому более низкая скорректированная оценка 
может указывать на то, что некоторые переменные не вносят должного вклада 
в R-квадрат вашей модели.
***
__F-statistics__ в линейной регрессии сравнивает полученную линейную модель
для переменных с моделью, которая заменяет влияние переменных на 0, 
чтобы выяснить, является ли ваша группа переменных статистически значимой.
Чтобы правильно интерпретировать это число, необходимо использовать 
выбранное значение альфа и F-таблицу.

__Prob (F-statistics)__ использует это число, чтобы сообщить вам точность
нулевой гипотезы или то, что влияние ваших переменных равно 0.
***
__Log-Likelihood (Логарифмическое правдоподобие)__ — это числовое значение 
вероятности того, что созданная модель произвела заданные данные. 
Он используется для сравнения значений коэффициентов для каждой переменной 
в процессе создания модели.
***
__AIC и BIC__ используются для сравнения эффективности моделей в процессе 
линейной регрессии с использованием штрафной системы для измерения 
нескольких переменных. Эти числа используются для выбора признаков переменных.
***
__Intercept__ является результатом нашей модели, если бы все переменные 
были бы настроены на 0. В классической линейной формуле 'y = mx+b' 
это наша b, константа, добавленная для объяснения начального значения нашей линии.
***
Наша первая информативная колонка – __(coef) коэффициент__.\
Для нашего перехвата это значение пересечения. 
Для каждой переменной это измерение того, как изменение этой переменной 
влияет на независимую переменную. Это 'm' в 'y = mx + b' Одна единица 
изменения зависимой переменной повлияет на величину изменения коэффициента
переменной в независимой переменной. Если коэффициент отрицательный, 
то они имеют обратную зависимость. Когда один поднимается, другой падает.
***
__std err (Стандартная ошибка)__ — это оценка стандартного отклонения 
коэффициента, измерение величины вариации коэффициента в точках его данных.\
__t__ является измерением точности, с которой был измерен коэффициент. 
Низкая ошибка std по сравнению с высоким коэффициентом дает высокую 
статистику t, что означает высокую значимость вашего коэффициента.
***
__P>|t|__ является одним из самых важных статистических показателей в сводке. 
Он использует t-статистику для получения значения p, измерения вероятности 
случайного измерения вашего коэффициента с помощью нашей модели. 
Значение p=0,378 независимой переменной говорит о том, что существует 37,8% 
вероятности того, что независимая переменная не влияет на нашу зависимую
переменную, и наши результаты получены случайно. При правильном анализе 
модели значение p сравнивается с ранее установленным альфа-значением, 
или порогом, с помощью которого мы можем применить значимость 
к нашему коэффициенту. Обычная альфа равна 0.05, которую в данном случае
передают немногие из наших переменных.
***
__[0.025 и 0.975] (Доверительные интервалы)__ — являются измерениями значений 
наших коэффициентов в пределах 95% наших данных, или в пределах двух стандартных
отклонений. За пределами этих значений, как правило, можно считать выбросами.
***
__Omnibus__ описывает нормальность распределения наших остатков, используя 
асимметрию и эксцесс в качестве измерений. 0 будет означать идеальную 
нормальность. __Prob(Omnibus)__ - это статистический тест, измеряющий 
вероятность нормального распределения остатков. Значение 1 указывает 
на совершенно нормальное распределение.
***
__Skew (Асимметрия)__ — это измерение симметрии в наших данных,
где 0 — идеальная симметрия. \
__Kurtosis (Эксцесс)__ измеряет пикичность наших данных, или их 
концентрацию около 0 на нормальной кривой. Чем выше эксцесс, 
тем меньше выбросов.
***
__Durbin-Watson (Метод Дурбина-Уотсона)__ — это измерение гомоскедастичности, 
или равномерного распределения ошибок по всем нашим данным. 
Гетероскедастичность подразумевает неравномерное распределение, 
например, чем выше точка данных, тем выше относительная погрешность. 
Идеальная гомоскедастичность будет лежать между 1 и 2.
***
__Jarque-Bera (JB) и Prob(JB)__ — являются альтернативными методами
измерения того же значения, что и Omnibus и Prob(Omnibus), 
используя асимметрию и эксцесс. Мы используем эти значения для
подтверждения друг друга.
***
__Cond. No. (Число обусловленности)__ — это измерение чувствительности 
нашей модели по сравнению с размером изменений в анализируемых ею данных.
Мультиколлинеарность в значительной степени подразумевается высоким числом 
обусловленности. Мультиколлинеарность Термин для описания двух или более 
независимых переменных, которые тесно связаны друг с другом и ложно 
влияют на прогнозируемую переменную избыточностью.
***
__Breusch-Pagan (Тест Бройша-Пагана)__ — тест на гетероскедастичность.
***
__Тест на мультиколлинеарность данных__
***
__Box Cox transformation (Преобразование Бокса-Кокса)__